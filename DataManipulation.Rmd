---
title: "GetData"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-packages, include=FALSE}
library(rvest)
library(XML)
library(RCurl)
library(tidyverse)
library(readr)
library(xgboost)
```


```{r getData, eval=FALSE}
results <- read.csv("Big_Dance_CSV.csv")
results <- subset(results, select = -Winner )

all_data <- read.csv("all_data.csv")
all_data <- all_data %>%
  mutate(higher_score = score, lower_score = score.1)
all_data <- subset(all_data, select = -score.1)

matchups <- read.csv("FirstRound2021.csv")
matchups2019 <- read.csv("Spreads2019.csv")

matchups <- matchups %>%
  mutate(lowerseed = if_else(lowerseed == "Oregon State", "Oregon St", lowerseed))

kenPomData <- data.frame()
inputPath <- "2019Data\\" # Location for storing the data to be used as an input to the ML work.

# For training purposes, get all data before the current year. Aggregate into a single data frame.

getKenPomYearData <- function(year)
{
    theUrl <- paste0("https://kenpom.com/index.php?y=", as.character(year))
    page <- read_html(theUrl)
    tables <- page %>% html_nodes("table") %>% html_table()
    data <- as.data.frame(tables[1])

    colnames(data) <- c("Rk", "Team", "Conf", "Record", "AdjEM", "AdjO", 
         "AdjO_R", "AdjD", "AdjD_R", "AdjT", "AdjT_R",
         "Luck", "Luck_R", "SoS_AdjEM", "SoS_AdjEM_R", 
         "OppO", "OppO_R", "OppD", "OppD_R", "NC_AdjEM", "NC_AdjEM_R")
    
    data$Year = year

    return(data)
}

for (year in 2002:2021)
{
    kenPomYear <- getKenPomYearData(year)
    kenPomData <- rbind(kenPomData, kenPomYear)
}

kenPomData2021 <- getKenPomYearData(2021)

names(kenPomData2021) <- tolower(names(kenPomData2021))
names(kenPomData)<-tolower(names(kenPomData))
names(results)<-tolower(names(results))
```

```{r cleaning, eval=FALSE}
kenPomData <- kenPomData %>%
  filter(rk != "Rk") %>%
  filter(!is.na(rk)) %>%
  filter(rk != "")

kenPomData2021 <- kenPomData2021 %>%
  filter(rk != "Rk") %>%
  filter(!is.na(rk)) %>%
  filter(rk != "")

kenPomData <- kenPomData %>%
  mutate(team = gsub("[.]", "", team))

kenPomData$team <- gsub('[0-9]+', '', kenPomData2021$team)
kenPomData$team <- trimws(kenPomData$team, "r")

kenPomData2021 <- kenPomData2021 %>%
  mutate(team = gsub("[.]", "", team))

kenPomData2021$team <- gsub('[0-9]+', '', kenPomData2021$team)
kenPomData2021$team <- trimws(kenPomData2021$team, "r")

kenPomData2021 <- kenPomData2021 %>% 
  select(team, adjo, adjd, adjt) %>%
  mutate(adjo = as.numeric(adjo), adjd = as.numeric
         (adjd), adjt = as.numeric(adjt))

kenPomData <- subset(kenPomData, select = -c(conf, nc_adjem, nc_adjem_r, sos_adjem, sos_adjem_r, oppo, oppo_r, oppd, oppd_r))

matchups <- matchups %>%
  mutate(higherseed = gsub("[.]", "", higherseed), lowerseed = gsub("[.]", "", lowerseed))

matchups2019 <- matchups2019 %>%
  mutate(higherseed = gsub("[.]", "", higherseed), lowerseed = gsub("[.]", "", lowerseed))  

matchups2019 <- matchups2019 %>%
  mutate(adjo.x = as.numeric(adjo.x), adjd.x = as.numeric
         (adjd.x), adjt.x = as.numeric(adjt.x), adjo.y = as.numeric(adjo.y), adjd.y = as.numeric
         (adjd.y), adjt.y = as.numeric(adjt.y))
```

```{r joinData, eval=FALSE}
real_data <- merge(results, kenPomData, by.x = c("year", "higherteam"), by.y = c("year", "team"))

kenPom2019 <- kenPomData %>%
  filter(year == 2019)

matchups <- merge(matchups, kenPomData2021, by.x = "higherseed", by.y = "team")
matchups <- merge(matchups, kenPomData2021, by.x = "lowerseed", by.y = "team")
matchups2019 <- merge(matchups2019, kenPom2019, by.x = "lowerseed", by.y = "team")
matchups2019 <- matchups2019 %>% unique()

all_data <- merge(all_data, kenPomData, by = c("year", "team"))

all_data <- all_data %>%
  rename(lowerteam = team)

kenPomData %>% group_by(team) %>% count()
results %>% group_by(team) %>% count()

all_data <- all_data %>%
  mutate(winner = if_else(higher_score > lower_score, "1", "0")) %>%
  mutate(winner = factor(winner))

write.csv(all_data,"all_data.csv", row.names = TRUE)

col_order <- c("round", "higherseed", "lowerseed", "higher_score", "lower_score", "higherteam", "lowerteam", "rk.x", "adjem.x", "adjo.x", "adjo_r.x", "adjd.x", "adjd_r.x", "adjt.x", "adjt_r.x", "luck.x", "luck_r.x", "rk.y", "adjem.y", "adjo.y", "adjo_r.y", "adjd.y", "adjd_r.y", "adjt.y", "adjt_r.y", "year")

all_data <- all_data[, col_order]

most_data <- all_data %>% filter(year != 2019)
data2019 <- all_data %>% filter(year == 2019)
```

```{r xgboost, eval=FALSE}
set.seed(33)

model_data <- all_data %>%
  mutate(diff_score = as.numeric(diff_score)) %>%
  select(diff_score, adjo.x, adjd.x, adjt.x, adjo.y, adjd.y, adjt.y)
model_data <- model_data[sample(1:nrow(model_data)), ]

labels <- model_data %>%
  select(diff_score)

model_data <- model_data %>%
  select(-diff_score)

final <- data.matrix(model_data)

numberOfTrainingSamples <- round(length(labels$diff_score) * 1)

train_data <- final[1:numberOfTrainingSamples,]
train_labels <- labels[1:numberOfTrainingSamples,]

test_data <- final[-(1:numberOfTrainingSamples),]
test_labels <- labels[-(1:numberOfTrainingSamples),]

dtrain <- xgb.DMatrix(data = train_data, label= train_labels)
dtest <- xgb.DMatrix(data = test_data, label= test_labels)

xgb <- xgboost(data = dtrain,  
                 nround = 6)

params <- list(booster = "gbtree", objective = "reg:squarederror", eta=0.3, gamma=0, max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1)

xgbcv <- xgb.cv(params = params, data = dtrain, nrounds = 3000, nfold = 5, showsd = T, stratified = T, print.every.n = 50, early.stop.round = 1000, maximize = F)

#min(xgbcv$test.error.mean)

importance_matrix <- xgb.importance(names(final), model = xgb)

xgb.plot.importance(importance_matrix)


```

```{r xgboost2, eval=FALSE}
set.seed(33)
model_data <- most_data %>%
  select(total, adjo.x, adjd.x, adjt.x, adjo.y, adjd.y, adjt.y)
#model_data <- model_data[sample(1:nrow(model_data)), ]

labels <- model_data %>%
  select(total)

model_data <- model_data %>%
  select(-total)

final <- data.matrix(model_data)

numberOfTrainingSamples <- round(length(labels$total) * .75)

train_data <- final[1:numberOfTrainingSamples,]
train_labels <- labels[1:numberOfTrainingSamples,]

test_data <- final[-(1:numberOfTrainingSamples),]
test_labels <- labels[-(1:numberOfTrainingSamples),]

dtrain <- xgb.DMatrix(data = train_data, label= train_labels)
dtest <- xgb.DMatrix(data = test_data, label= test_labels)

xgb <- xgboost(data = dtrain,  
                 nround = 13)

params_total <- list(booster = "gbtree", objective = "reg:squarederror", eta=0.3, gamma=0, max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1)

xgbcv <- xgb.cv(params = params_total, data = dtrain, nrounds = 3000, nfold = 5, showsd = T, stratified = T, print.every.n = 20, early.stop.round = 40, maximize = F)


importance_matrix <- xgb.importance(names(final), model = xgb_tuned)

xgb.plot.importance(importance_matrix)


total_model_data <- matchups %>%
  select(adjo.x, adjd.x, adjt.x, adjo.y, adjd.y, adjt.y)

total_test_data2021 <- as.matrix(total_model_data)

dtest <- xgb.DMatrix(data = total_test_data2021)

total_model_data2 <- total_model_data %>%
  mutate(pred_total = data.frame(predict(xgb, dtest)))

matchups_with_pred_total <- matchups %>%
  mutate(pred_total = total_model_data2$pred_total) %>%
  unique()
```


```{r makeTunedModel, eval=FALSE}
set.seed(33)
params <- list(booster = "gbtree", 
               objective = "reg:squarederror",
               max_depth = randomsearch[1,]$max_depth,
               eta = randomsearch[1,]$eta,
               subsample = randomsearch[1,]$subsample,
               colsample_bytree = randomsearch[1,]$colsample_bytree,
               min_child_weight = randomsearch[1,]$min_child_weight)
xgb_tuned <- xgboost(params = params, data = dtrain, nrounds =15)
```

```{r tuning, eval=FALSE}
# Create empty lists
lowest_error_list = list()
parameters_list = list()

# Create 10,000 rows with random hyperparameters
set.seed(33)
for (iter in 1:500){
  param <- list(booster = "gbtree",
                objective = "reg:squarederror",
                max_depth = sample(3:10, 1),
                eta = runif(1, .01, .3),
                subsample = runif(1, .7, 1),
                colsample_bytree = runif(1, .6, 1),
                min_child_weight = sample(0:10, 1)
  )
  parameters <- as.data.frame(param)
  parameters_list[[iter]] <- parameters
}

# Create object that contains all randomly created hyperparameters
parameters_df = do.call(rbind, parameters_list)

# Use randomly created parameters to create 10,000 XGBoost-models
for (row in 1:nrow(parameters_df)){
  set.seed(33)
  mdcv <- xgb.train(data=dtrain,
                    booster = "gbtree",
                    objective = "reg:squarederror",
                    max_depth = parameters_df$max_depth[row],
                    eta = parameters_df$eta[row],
                    subsample = parameters_df$subsample[row],
                    colsample_bytree = parameters_df$colsample_bytree[row],
                    min_child_weight = parameters_df$min_child_weight[row],
                    nrounds= 300,
                    eval_metric = "error",
                    early_stopping_rounds= 30,
                    print_every_n = 100,
                    watchlist = list(train= dtrain, val= dtest)
  )
  lowest_error <- as.data.frame(1 - min(mdcv$evaluation_log$val_error))
  lowest_error_list[[row]] <- lowest_error
}

# Create object that contains all accuracy's
lowest_error_df = do.call(rbind, lowest_error_list)

# Bind columns of accuracy values and random hyperparameter values
randomsearch = cbind(lowest_error_df, parameters_df)

# Quickly display highest accuracy
max(randomsearch$`1 - min(mdcv$evaluation_log$val_error)`)

randomsearch <- as.data.frame(randomsearch) %>%
  rename(val_acc = `1 - min(mdcv$evaluation_log$val_error)`) %>%
  arrange(-val_acc)
```

```{r, eval=FALSE}
for(x in 1:200){
  xgb <- xgboost(data = dtrain,  
                 nround = x)
  pred <- predict(xgb, dtest)
  err <- median(pred - test_labels)
  print(paste("test-error=", err))
}

test_labels <- data.frame(test_labels)
test_labels <- test_labels %>%
  mutate(pred_diff = predict(xgb, dtest)) %>%
  mutate(correct = if_else((test_labels > 0 & pred_diff > 0) | (test_labels < 0 & pred_diff < 0), TRUE, FALSE)) %>% 
  mutate(residual = test_labels - pred_diff)

median(test_labels$residual)

data2019 <- data2019 %>%
  mutate(total = higher_score + lower_score)


test_data2019 <- data2019 %>%
  select(adjo.x, adjd.x, adjt.x, adjo.y, adjd.y, adjt.y, total)
test_data2019_matrix <- as.matrix(test_data2019)

dtest <- xgb.DMatrix(data = test_data2019_matrix)

new_data2 <- test_data2019 %>%
  mutate(pred_diff = data.frame(predict(xgb_tuned, dtest))) %>% 
  mutate(pred_total = data.frame(predict(xgb, dtest))) %>%
  unique()

matchups_with_pred2019 <- data2019 %>%
  mutate(pred_diff = new_data2$pred_diff)

matchups_with_pred2019 <- matchups_with_pred2019 %>%
  mutate(score_diff = higher_score - lower_score) %>%
  mutate(correct = if_else((score_diff > 0 & pred_diff > 0) | (score_diff < 0 & pred_diff < 0), TRUE, FALSE)) %>%
  mutate(residual = score_diff - pred_diff)
         
matchups_with_pred2019 %>% count(correct)
matchups_with_pred %>% mean(residual.predict.xgb..dtest.)


matchups_with_pred1 <- matchups_with_pred[-nrow(matchups_with_pred),]


new_model_data <- matchups %>%
  select(adjo.x, adjd.x, adjt.x, adjo.y, adjd.y, adjt.y)

test_data2021 <- as.matrix(new_model_data)

dtest <- xgb.DMatrix(data = test_data2021)

new_data2 <- new_model_data %>%
  mutate(pred_diff = data.frame(predict(xgb_tuned, dtest)))

matchups_with_pred <- matchups %>%
  mutate(pred_diff = new_data2$pred_diff) %>%
  unique()



mich = c(107.5, 93.8, 68.6, 114.2, 97.9, 64.6)
norfolk = c(99.5, 103.2, 67.6, 98.4, 102.8, 65.7)
texsouth = c(99.3, 104.4, 71.7, 94.7, 100.8, 61.9)
drake = c(109.0, 97.2, 67.3, 112.8, 98.3, 66.6)

today <- data.frame(rbind(mich, norfolk, texsouth, drake)) %>%
  rename(adjo.x = X1, adjd.x = X2, adjt.x = X3, adjo.y = X4, adjd.y = X5, adjt.y = X6)

dtest <- xgb.DMatrix(data = as.matrix(today))

today_test <- data.frame(predict(xgb_tuned, dtest)) %>% unique()

matchups_with_pred45 <- matchups %>%
  mutate(pred_diff = new_data2$pred_diff)
```

```{r test2019, eval=FALSE}
set.seed(33)

model_data <- most_data %>%
  mutate(diff_score = as.numeric(diff_score)) %>%
  select(diff_score, adjo.x, adjd.x, adjt.x, adjo.y, adjd.y, adjt.y)
model_data <- model_data[sample(1:nrow(model_data)), ]

labels <- model_data %>%
  select(diff_score)

model_data <- model_data %>%
  select(-diff_score)

final <- data.matrix(model_data)

numberOfTrainingSamples <- round(length(labels$diff_score) * 1)

train_data <- final[1:numberOfTrainingSamples,]
train_labels <- labels[1:numberOfTrainingSamples,]

test_data <- final[-(1:numberOfTrainingSamples),]
test_labels <- labels[-(1:numberOfTrainingSamples),]

dtrain <- xgb.DMatrix(data = train_data, label= train_labels)
dtest <- xgb.DMatrix(data = test_data, label= test_labels)

xgb <- xgboost(data = dtrain,  
               nround = 13)

matchups2019 <- matchups2019 %>% unique()

test_data2019 <- matchups2019 %>%
  select(adjo.x, adjd.x, adjt.x, adjo.y, adjd.y, adjt.y)
test_data2019_matrix <- as.matrix(test_data2019)

dtest <- xgb.DMatrix(data = test_data2019_matrix)
#data.frame(predict(xgb_tuned, dtest))

#new_data2 <- test_data2019 %>%
#  mutate(pred_diff = data.frame(predict(xgb, dtest))) %>% unique() 

matchups_with_pred2019 <- matchups2019 %>%
  mutate(pred_total = data.frame(predict(xgb, dtest)))

matchups_with_pred2019$correct_spread <- if_else((matchups_with_pred2019[,ncol(matchups_with_pred2019)] > matchups_with_pred2019$spread & matchups_with_pred2019$covered == "Yes") | (matchups_with_pred2019[,ncol(matchups_with_pred2019)] < matchups_with_pred2019$spread & matchups_with_pred2019$covered == "No"), "Yes", "No")

matchups_with_pred2019 %>% count(correct_spread)
```
